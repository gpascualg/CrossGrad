{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB Loading won't be available\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from SenseTheFlow import config\n",
    "config.bar = tqdm.tqdm_notebook\n",
    "\n",
    "from SenseTheFlow.model import Model, CustomSummarySaverHook, DataParser, EvalCallback\n",
    "from SenseTheFlow import rocksdb\n",
    "from SenseTheFlow.dataset import Dataset, CustomLoader\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "import random\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "\n",
    "def train_generator():\n",
    "    images, labels = mnist.train_images(), mnist.train_labels()\n",
    "    for i in range(images.shape[0]):\n",
    "        img, label = images[i, :, :, np.newaxis], labels[i]\n",
    "        img = img.astype(np.float32)\n",
    "        cls = random.randint(0, 1)\n",
    "        \n",
    "        if cls == 1:\n",
    "            img = rotate(img, 15, reshape=False)\n",
    "            \n",
    "        yield (img, {'label': [int(label)], 'cls': [int(cls)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossgrad_latent_fn(features, labels, mode, params):\n",
    "    # Some convolutions\n",
    "    features = tf.layers.conv2d(features, 32, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.conv2d(features, 32, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.max_pooling2d(features, 2, 1, data_format=params['data_format'])\n",
    "    features = tf.layers.conv2d(features, 128, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.conv2d(features, 128, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.max_pooling2d(features, 2, 1, data_format=params['data_format'])\n",
    "    features = tf.layers.conv2d(features, 256, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.conv2d(features, 256, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    \n",
    "    # Down to [batch, 1, 1, 256] -> [batch, 1, 1, 256]\n",
    "    size = features.shape.as_list()[2]\n",
    "    features = tf.layers.max_pooling2d(features, size, 1, data_format=params['data_format'])\n",
    "\n",
    "    # Target dimensions [batch, 1, 1, target] -> [batch, target]\n",
    "    features = tf.layers.conv2d(features, params['latent_space_dimensions'], 1, data_format=params['data_format'])\n",
    "    features = tf.layers.flatten(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossgrad_domain_fn(features, labels, mode, params):\n",
    "    # Compute logits\n",
    "    features = tf.layers.dense(features, params['latent_space_dimensions'], activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(features, params['num_domain'])\n",
    "    \n",
    "    # Final classification\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    domain = tf.argmax(softmax, axis=-1)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(labels, params['num_domain']),\n",
    "        logits=logits\n",
    "    )\n",
    "    \n",
    "    return domain, tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossgrad(mode, label_fn, x, domain, labels, epsilon_d, epsilon_l, alpha_d, alpha_l, latent_fn=None, domain_fn=None, latent_space_dimensions=100, params=None):    \n",
    "    # Some default parameters\n",
    "    if params == None: params = {}        \n",
    "    params['latent_space_dimensions'] = latent_space_dimensions\n",
    "    if 'data_format' not in params or params['data_format'] is None:\n",
    "        params['data_format'] = detect_data_format()\n",
    "    \n",
    "    # Given functions or default\n",
    "    latent_fn = latent_fn or crossgrad_latent_fn\n",
    "    domain_fn = domain_fn or crossgrad_domain_fn\n",
    "    \n",
    "    # To expected data format\n",
    "    x = to_data_format(x, 'channels_last', params['data_format'])\n",
    "    \n",
    "    def forward(x_l, x_d, d, labels, mode, params):\n",
    "        latent = latent_fn(features=x_l, labels=None, mode=mode, params=params)\n",
    "        domain, loss_domain = domain_fn(features=latent, labels=d, mode=mode, params=params)\n",
    "        outputs, loss_label = label_fn(features=x_d, latent=latent, labels=labels, mode=mode, params=params)\n",
    "\n",
    "        return ((domain, loss_domain), (outputs, loss_label))\n",
    "    \n",
    "    # First forward pass\n",
    "    ((d1, d1_loss), (l1, l1_loss)) = forward(x, x, domain, labels, mode, params)\n",
    "    grad_label_x = tf.gradients(l1_loss, x)[0]\n",
    "    grad_domain_x = tf.gradients(d1_loss, x)[0]\n",
    "    \n",
    "    tf.summary.scalar('loss/domain/1', d1_loss)\n",
    "    tf.summary.scalar('loss/labels/1', l1_loss)\n",
    "    \n",
    "    # Second pass\n",
    "    x_d = x + epsilon_d * grad_domain_x\n",
    "    x_l = x + epsilon_l * grad_label_x\n",
    "    ((d2, d2_loss), (l2, l2_loss)) = forward(x_l, x_d, domain, labels, mode, params)\n",
    "    \n",
    "    tf.summary.scalar('loss/domain/2', d2_loss)\n",
    "    tf.summary.scalar('loss/labels/2', l2_loss)    \n",
    "    \n",
    "    tf.summary.image('x', to_data_format(x, params['data_format'], 'channels_last'))\n",
    "    tf.summary.image('x_d', to_data_format(x_d, params['data_format'], 'channels_last'))\n",
    "    tf.summary.image('x_l', to_data_format(x_l, params['data_format'], 'channels_last'))\n",
    "    \n",
    "    l_total_loss = (1 - alpha_l) * l1_loss + alpha_l * l2_loss\n",
    "    d_total_loss = (1 - alpha_d) * d1_loss + alpha_d * d2_loss\n",
    "    predictions = {\n",
    "        'd1': d1,\n",
    "        'd2': d2,\n",
    "        'l1': l1,\n",
    "        'l2': l2\n",
    "    }\n",
    "    return predictions, l_total_loss + d_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_fn(features, latent, labels, mode, params):\n",
    "    features = tf.layers.conv2d(features, 32, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.conv2d(features, 64, 3, data_format=params['data_format'], activation=tf.nn.relu)\n",
    "    features = tf.layers.max_pooling2d(features, 2, 1, data_format=params['data_format'])\n",
    "    features = tf.layers.flatten(features)\n",
    "    features = tf.layers.dense(features, params['latent_space_dimensions'], activation=tf.nn.relu)\n",
    "    features = tf.concat((features, latent), axis=-1)\n",
    "    features = tf.layers.dense(features, params['latent_space_dimensions'], activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(features, params['num_classes'])\n",
    "    \n",
    "    print('LOGITS_LABELS')\n",
    "    print(logits)\n",
    "    \n",
    "    # Final classification\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    cls = tf.argmax(softmax, axis=-1)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(labels, params['num_classes']),\n",
    "        logits=logits\n",
    "    )\n",
    "    \n",
    "    return cls, tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    print(features, labels)\n",
    "    with tf.device('/gpu:1'):\n",
    "        predictions, loss = crossgrad(mode, label_fn, features, labels['cls'], labels['label'], 2.0, 2.0, 0.2, 0.2, params=params)\n",
    "        \n",
    "        print('done')\n",
    "        print(predictions['d1'])\n",
    "        print(labels['cls'])\n",
    "        print(predictions['d1'])\n",
    "        print(labels['cls'])\n",
    "        \n",
    "        accuracy_d1 = tf.metrics.accuracy(predictions['d1'], labels['cls'])\n",
    "        accuracy_l1 = tf.metrics.accuracy(predictions['l1'], labels['label'])\n",
    "        tf.summary.scalar('metrics/epoch/accuracy/domain/1', accuracy_d1[1])\n",
    "        tf.summary.scalar('metrics/epoch/accuracy/labels/1', accuracy_l1[1])\n",
    "        \n",
    "        accuracy_d2 = tf.metrics.accuracy(predictions['d2'], labels['cls'])\n",
    "        accuracy_l2 = tf.metrics.accuracy(predictions['l2'], labels['label'])\n",
    "        tf.summary.scalar('metrics/epoch/accuracy/domain/2', accuracy_d2[1])\n",
    "        tf.summary.scalar('metrics/epoch/accuracy/labels/2', accuracy_l2[1])\n",
    "        \n",
    "        \n",
    "        # Fetch global step\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        tf.summary.scalar('global_step/step', global_step)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "            # Batch norm requires update_ops to be added as a train_op dependency.\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops + [accuracy_d1[0], accuracy_l1[0], accuracy_d2[0], accuracy_l2[0]]):\n",
    "                train_op = optimizer.minimize(loss, global_step)\n",
    "        else:\n",
    "            train_op = None\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops={\n",
    "                'accuracy/domain/1': accuracy_d1, \n",
    "                'accuracy/labels/1': accuracy_l1,\n",
    "                'accuracy/domain/2': accuracy_d2, \n",
    "                'accuracy/labels/2': accuracy_l2\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12881bbd2ccb46cf862da55611c15450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b3718e51a4b8293beb31cfae73605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "You have no `evaluation` dataset\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "You have no `evaluation` dataset\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "You have no `evaluation` dataset\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "You have no `evaluation` dataset\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "You have no `evaluation` dataset\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0) {'label': <tf.Tensor 'IteratorGetNext:2' shape=(?, 1) dtype=int32>, 'cls': <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int32>}\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_4/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "LOGITS_LABELS\n",
      "Tensor(\"dense_9/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:1)\n",
      "done\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n",
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64, device=/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1), dtype=int32, device=/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'data_format': None,\n",
    "    'batch_size': 32,\n",
    "    'num_domain': 2,\n",
    "    'num_classes': 10\n",
    "}\n",
    "\n",
    "model_dir = '/data/storage/deepglobe/models/{}'.format('crossgrad')\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with Model(model_fn, model_dir, config=config, params=params, delete_existing=True) as model:\n",
    "    data_parser = DataParser()\n",
    "    data_parser.train_from_generator(\n",
    "        generator=train_generator,\n",
    "        output_types=(tf.float32, {'label': tf.int32, 'cls': tf.int32}),\n",
    "        output_shapes=([28, 28, 1], {'label': [1], 'cls': [1]}),\n",
    "        pre_shuffle=7,\n",
    "        post_shuffle=False,\n",
    "        flatten=False,\n",
    "        num_samples=None,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "     \n",
    "    model.data(data_parser)\n",
    "    \n",
    "    model.train(100, 2, eval_summary=[\n",
    "        'metrics/batch/accuracy', 'metrics/epoch/accuracy', 'metrics/epoch/miou'\n",
    "    ])\n",
    "\n",
    "#     do_predict(model)\n",
    "#     do_train_pred(model)\n",
    "#     do_eval_pred(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
